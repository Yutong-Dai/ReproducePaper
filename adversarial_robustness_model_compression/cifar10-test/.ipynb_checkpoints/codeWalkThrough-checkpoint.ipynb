{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "sys.path.append('../../') # append root directory\n",
    "import os\n",
    "import argparse\n",
    "from cifar10.utils import getLogger\n",
    "from cifar10.models import ResNet18_wby16\n",
    "from cifar10.config import Config\n",
    "from admm.warmup_scheduler import GradualWarmupScheduler\n",
    "from admm.cross_entropy import CrossEntropyLossMaybeSmooth\n",
    "from admm.utils import mixup_data, mixup_criterion\n",
    "import admm\n",
    "import torch.optim as optim\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get configuration\n",
    "parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
    "parser.add_argument('--config_file', type=str, default='./cifar10/config.yaml', help =\"config file\")\n",
    "parser.add_argument('--stage', type=str, default='admm', help =\"select the pruning stage\")\n",
    "args = parser.parse_args(\"\")\n",
    "config = Config(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pretrain', False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.stage, config.smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "under stand the mixup data augumentation and mixup loss function\n",
    "https://github.com/facebookresearch/mixup-cifar10/issues/18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "if config.logging:\n",
    "    log_dir = config.log_dir\n",
    "    logger = getLogger(log_dir)\n",
    "    logger.info(json.dumps(config.__dict__, indent=4))\n",
    "else:\n",
    "    logger = None\n",
    "\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=config.workers)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=config.workers)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print('==> Building model..')\n",
    "model = None\n",
    "if config.arch == \"vgg16\":\n",
    "    model = VGG('vgg16', w= config.width_multiplier)\n",
    "elif config.arch ==\"resnet18_wby16\":\n",
    "    model = ResNet18_wby16(config.w)\n",
    "config.model = model\n",
    "\n",
    "if device == 'cuda':\n",
    "    if config.gpu is not None:\n",
    "        torch.cuda.set_device(config.gpu)\n",
    "        config.model = torch.nn.DataParallel(model,device_ids = [config.gpu])\n",
    "    else:\n",
    "        config.model.cuda()\n",
    "        config.model = torch.nn.DataParallel(model)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "if config.load_model:\n",
    "    # unlike resume, load model does not care optimizer status or start_epoch\n",
    "    config.load_model.replace('w', str(config.w))\n",
    "    print('==> Loading from {}'.format(config.load_model))\n",
    "\n",
    "    config.model.load_state_dict(torch.load(config.load_model)) # i call 'net' \"model\"\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "config.prepare_pruning() # take the model and prepare the pruning\n",
    "\n",
    "ADMM = None\n",
    "\n",
    "if config.admm:\n",
    "    ADMM = admm.ADMM(config, device)\n",
    "\n",
    "\n",
    "\n",
    "if config.resume:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/ckpt.t7')\n",
    "    config.model.load_state_dict(checkpoint['net'])\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    ADMM.ADMM_U = checkpoint['admm']['ADMM_U']\n",
    "    ADMM.ADMM_Z = checkpoint['admm']['ADMM_Z']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLossMaybeSmooth(smooth_eps=config.smooth_eps).cuda(config.gpu)\n",
    "config.smooth = config.smooth_eps > 0.0\n",
    "config.mixup = config.alpha > 0.0\n",
    "\n",
    "\n",
    "config.warmup = (not config.admm) and config.warmup_epochs > 0\n",
    "optimizer_init_lr = config.warmup_lr if config.warmup else config.lr\n",
    "\n",
    "optimizer = None\n",
    "if (config.optimizer == 'sgd'):\n",
    "    optimizer = torch.optim.SGD(config.model.parameters(), optimizer_init_lr,\n",
    "                            momentum=0.9,\n",
    "                                weight_decay=1e-4)\n",
    "elif (config.optimizer =='adam'):\n",
    "    optimizer = torch.optim.Adam(config.model.parameters(), optimizer_init_lr)\n",
    "    \n",
    "scheduler = None\n",
    "if config.lr_scheduler == 'cosine':\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs*len(trainloader), eta_min=4e-08)\n",
    "elif config.lr_scheduler == 'default':\n",
    "    # my learning rate scheduler for cifar, following https://github.com/kuangliu/pytorch-cifar\n",
    "    epoch_milestones = [150, 250, 350]\n",
    "\n",
    "    \"\"\"Set the learning rate of each parameter group to the initial lr decayed\n",
    "        by gamma once the number of epoch reaches one of the milestones\n",
    "    \"\"\"\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[i*len(trainloader) for i in epoch_milestones], gamma=0.1)\n",
    "else:\n",
    "    raise Exception(\"unknown lr scheduler\")\n",
    "\n",
    "if config.warmup:\n",
    "    scheduler = GradualWarmupScheduler(optimizer, multiplier=config.lr/config.warmup_lr, total_iter=config.warmup_epochs*len(trainloader), after_scheduler=scheduler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('adam', False, 'resnet18_wby16_pretrained.pt')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.optimizer, config.masked_retrain, config.save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
